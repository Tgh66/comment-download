import streamlit as st
import asyncio
import pandas as pd
import re
import time
import requests
import json
import urllib.parse
import io 
import os
import math  # æ–°å¢ï¼šç”¨äºè®¡ç®—é¡µæ•°
from bilibili_api import video, comment, Credential
from bilibili_api.exceptions import ResponseCodeException

# --- PDF ç”Ÿæˆç›¸å…³åº“ ---
from reportlab.lib.pagesizes import A4
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.cidfonts import UnicodeCIDFont 
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib import colors

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="Bç«™è¯„è®ºæŠ“å–ç¥å™¨ (å¹¶å‘ç‰ˆ)", page_icon="âš¡", layout="wide")

# --- åˆå§‹åŒ– Session State ---
if 'comments_data' not in st.session_state:
    st.session_state.comments_data = None
if 'video_title' not in st.session_state:
    st.session_state.video_title = ""
if 'bv_id' not in st.session_state:
    st.session_state.bv_id = ""

# --- è¾…åŠ©å‡½æ•° ---

def get_real_url(url):
    """å¤„ç† b23.tv çŸ­é“¾æ¥"""
    if "b23.tv" in url:
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            resp = requests.get(url, headers=headers, allow_redirects=True, timeout=10)
            return resp.url
        except:
            return url
    return url

def extract_bv(url):
    """æå–BVå·"""
    real_url = get_real_url(url)
    pattern = r"(BV[a-zA-Z0-9]{10})"
    match = re.search(pattern, real_url)
    if match:
        return match.group(1), real_url
    return None, real_url

def parse_cookie_json(json_str):
    """è§£æç”¨æˆ·ç²˜è´´çš„ JSON Cookie æ•°æ®"""
    try:
        data = json.loads(json_str)
        
        cookie_list = []
        if isinstance(data, list):
            cookie_list = data
        elif isinstance(data, dict) and "cookies" in data:
            cookie_list = data["cookies"]
        else:
            return None, "JSON æ ¼å¼ä¸æ­£ç¡®ï¼Œæœªæ‰¾åˆ° cookies åˆ—è¡¨"

        cookies = {c['name']: c['value'] for c in cookie_list}
        
        sessdata = cookies.get('SESSDATA')
        bili_jct = cookies.get('bili_jct')
        buvid3 = cookies.get('buvid3')

        if not sessdata or not bili_jct:
            return None, "Cookie ä¸­ç¼ºå°‘ SESSDATA æˆ– bili_jct"

        sessdata = urllib.parse.unquote(sessdata)
        bili_jct = urllib.parse.unquote(bili_jct)

        cred = Credential(sessdata=sessdata, bili_jct=bili_jct, buvid3=buvid3)
        return cred, None

    except json.JSONDecodeError:
        return None, "JSON è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥å¤åˆ¶æ˜¯å¦å®Œæ•´"
    except Exception as e:
        return None, f"Cookie è§£æé”™è¯¯: {str(e)}"

# --- PDF ç”Ÿæˆå‡½æ•° (è‡ªåŠ¨æ¢è¡Œ + å®Œæ•´æ˜¾ç¤ºç‰ˆ) ---
def create_pdf(dataframe, title):
    """
    å°† DataFrame è½¬æ¢ä¸º PDF å­—èŠ‚æµ (æ”¯æŒè‡ªåŠ¨æ¢è¡Œ)
    """
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4)
    elements = []

    # 1. æ³¨å†Œ CID ä¸­æ–‡å­—ä½“
    font_name = 'STSong-Light'
    try:
        pdfmetrics.registerFont(UnicodeCIDFont('STSong-Light'))
    except Exception as e:
        font_name = "Helvetica"

    # 2. å®šä¹‰æ ·å¼
    styles = getSampleStyleSheet()
    
    # æ ‡é¢˜æ ·å¼
    title_style = styles['Title']
    if font_name == 'STSong-Light':
        title_style.fontName = font_name
    
    # æ­£æ–‡å†…å®¹æ ·å¼ (ç”¨äºè¡¨æ ¼å†…çš„é•¿æ–‡æœ¬è‡ªåŠ¨æ¢è¡Œ)
    cell_style = ParagraphStyle(
        name='CellStyle',
        fontName=font_name,
        fontSize=9,
        leading=12,
        wordWrap='CJK' # æ”¯æŒä¸­æ–‡æ–­è¡Œ
    )

    safe_title = re.sub(r'[^\w\s\u4e00-\u9fa5]', '', title)
    elements.append(Paragraph(f"è§†é¢‘è¯„è®º: {safe_title}", title_style))
    elements.append(Paragraph("<br/><br/>", styles['Normal']))

    # 3. å‡†å¤‡è¡¨æ ¼æ•°æ®
    col_widths = [70, 240, 40, 80, 40] 

    # å¤„ç†è¡¨å¤´
    headers = dataframe.columns.to_list()
    processed_data = [headers]

    # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
    for index, row in dataframe.iterrows():
        new_row = []
        
        uname = str(row['ç”¨æˆ·å'])
        content = str(row['å†…å®¹'])
        like = str(row['ç‚¹èµ'])
        time_str = str(row['æ—¶é—´'])
        reply_count = str(row['å›å¤æ•°'])

        # æ¸…ç† PDF ä¸æ”¯æŒçš„å­—ç¬¦
        content = re.sub(r'[^\x00-\x7F\u4e00-\u9fa5]+', '', content)
        uname = re.sub(r'[^\x00-\x7F\u4e00-\u9fa5]+', '', uname)

        # è½¬æ¢ä¸º Paragraph å¯¹è±¡
        new_row.append(Paragraph(uname, cell_style)) 
        new_row.append(Paragraph(content, cell_style))
        new_row.append(like)
        new_row.append(time_str) 
        new_row.append(reply_count)

        processed_data.append(new_row)

    # 4. åˆ›å»ºè¡¨æ ¼å¯¹è±¡
    t = Table(processed_data, colWidths=col_widths)
    
    # 5. è®¾ç½®è¡¨æ ¼æ ·å¼
    style = TableStyle([
        ('FONTNAME', (0, 0), (-1, -1), font_name), 
        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
        ('ALIGN', (0, 0), (-1, 0), 'CENTER'),
        ('VALIGN', (0, 0), (-1, -1), 'TOP'),
        ('FONTSIZE', (0, 0), (-1, 0), 10),
        ('FONTSIZE', (0, 0), (-1, -1), 9),
        ('GRID', (0, 0), (-1, -1), 0.5, colors.black),
        ('LEFTPADDING', (0, 0), (-1, -1), 4),
        ('RIGHTPADDING', (0, 0), (-1, -1), 4),
        ('TOPPADDING', (0, 0), (-1, -1), 4),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 4),
    ])
    t.setStyle(style)
    elements.append(t)

    # 6. ç”Ÿæˆ PDF
    try:
        doc.build(elements)
        buffer.seek(0)
        return buffer
    except Exception as e:
        print(f"PDFç”Ÿæˆé”™è¯¯: {e}")
        return None

# ğŸ‘‡ è‡ªå®šä¹‰ç±»
class VideoTypeFix:
    value = 1 

# --- æ–°å¢ï¼šå¤„ç†å•é¡µæŠ“å–çš„åŒ…è£…å‡½æ•° ---
async def fetch_one_page(oid, page, credential, semaphore):
    """
    å•ä¸ªé¡µé¢æŠ“å–ä»»åŠ¡ï¼Œå—ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
    """
    async with semaphore:  # é™åˆ¶åŒæ—¶è¿è¡Œçš„ä»»åŠ¡æ•°é‡
        try:
            # éšæœºçŸ­æš‚ä¼‘çœ ï¼Œé˜²æ­¢è§¦å‘ B ç«™é£æ§
            await asyncio.sleep(0.05)
            c = await comment.get_comments(oid, VideoTypeFix(), page, credential=credential)
            return c
        except Exception as e:
            return None

async def fetch_comments_async(bv_id, fetch_mode, limit_pages, credential=None):
    """
    å¼‚æ­¥å¹¶å‘æŠ“å–è¯„è®º (æ ¸å¿ƒé‡æ„)
    """
    v = video.Video(bvid=bv_id, credential=credential)
    
    try:
        info = await v.get_info()
        oid = info['aid']
        title = info['title']
    except Exception as e:
        return None, f"æ— æ³•è·å–è§†é¢‘ä¿¡æ¯: {str(e)}"

    comments_data = []
    
    # è¿›åº¦æ˜¾ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()
    status_text.text("ğŸš€ æ­£åœ¨åˆå§‹åŒ–...")

    # --- ç¬¬ä¸€æ­¥ï¼šæŠ“å–ç¬¬1é¡µï¼Œè·å–æ€»é¡µæ•°ä¿¡æ¯ ---
    try:
        page_1_data = await comment.get_comments(oid, VideoTypeFix(), 1, credential=credential)
    except ResponseCodeException as e:
        return None, f"æŠ“å–ç¬¬1é¡µå¤±è´¥ï¼Œé”™è¯¯ç : {e.code}"
    
    if not page_1_data:
        return title, []

    # è®¡ç®—æ€»é¡µæ•°
    page_info = page_1_data.get('page', {})
    total_count = page_info.get('count', 0)
    total_pages_available = math.ceil(total_count / 20) # Bç«™æ¯é¡µ20æ¡
    
    # ç¡®å®šç›®æ ‡æŠ“å–é¡µæ•°
    if fetch_mode == "å…¨éƒ¨ä¸‹è½½":
        target_pages = total_pages_available
        status_text.text(f"æ£€æµ‹åˆ°å…± {total_count} æ¡è¯„è®ºï¼Œçº¦ {target_pages} é¡µï¼Œå‡†å¤‡å…¨éƒ¨ä¸‹è½½...")
    else:
        target_pages = min(total_pages_available, limit_pages)
        status_text.text(f"å‡†å¤‡ä¸‹è½½å‰ {target_pages} é¡µ...")

    # å…ˆå¤„ç†ç¬¬1é¡µçš„æ•°æ®
    def process_comments_json(c_json):
        processed = []
        if 'replies' not in c_json or not c_json['replies']:
            return processed
            
        for r in c_json['replies']:
            item = {
                'ç”¨æˆ·å': r['member']['uname'],
                'å†…å®¹': r['content']['message'],
                'ç‚¹èµ': int(r['like']), 
                'æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(r['ctime'])),
                'å›å¤æ•°': int(r['count'])
            }
            processed.append(item)
            if r.get('replies'):
                for sub in r['replies']:
                    sub_item = {
                        'ç”¨æˆ·å': sub['member']['uname'],
                        'å†…å®¹': f"[å›å¤] {sub['content']['message']}",
                        'ç‚¹èµ': int(sub['like']),
                        'æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(sub['ctime'])),
                        'å›å¤æ•°': 0
                    }
                    processed.append(sub_item)
        return processed

    comments_data.extend(process_comments_json(page_1_data))
    progress_bar.progress(1 / max(target_pages, 1))

    # --- ç¬¬äºŒæ­¥ï¼šå¹¶å‘æŠ“å–å‰©ä½™é¡µé¢ (å¦‚æœæœ‰) ---
    if target_pages > 1:
        # é™åˆ¶å¹¶å‘æ•°ä¸º 5 (å¤ªé«˜ä¼šè¢«å°)
        sem = asyncio.Semaphore(5)
        tasks = []
        
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨ (ä»ç¬¬2é¡µå¼€å§‹)
        for p in range(2, target_pages + 1):
            task = fetch_one_page(oid, p, credential, sem)
            tasks.append(task)
        
        # è¿è¡Œå¹¶å‘ä»»åŠ¡
        finished_count = 1 # å·²ç»æŠ“äº†ç¬¬1é¡µ
        
        # as_completed å…è®¸æˆ‘ä»¬æ¯å®Œæˆä¸€ä¸ªä»»åŠ¡å°±æ›´æ–°ä¸€æ¬¡ UI
        for future in asyncio.as_completed(tasks):
            result = await future
            finished_count += 1
            
            if result:
                new_items = process_comments_json(result)
                comments_data.extend(new_items)
            
            # æ›´æ–°è¿›åº¦æ¡
            progress = min(finished_count / target_pages, 1.0)
            progress_bar.progress(progress)
            status_text.text(f"âš¡ æ­£åœ¨å¹¶å‘ä¸‹è½½: {finished_count}/{target_pages} é¡µ...")

    status_text.text("âœ… ä¸‹è½½å®Œæˆï¼")
    await asyncio.sleep(0.5)
    
    return title, comments_data

# --- UI å¸ƒå±€ ---

st.title("âš¡ Bç«™è¯„è®ºæŠ“å– (å¹¶å‘ä¸‹è½½+å…¨é‡ç‰ˆ)")

with st.sidebar:
    st.header("ğŸ” èº«ä»½éªŒè¯ (æ¨è)")
    st.info("ç²˜è´´ Cookie JSON")
    
    cookie_input = st.text_area(
        "Cookie æ•°æ®:", 
        height=150,
        placeholder='{"url": "...", "cookies": [...]}'
    )
    
    cred = None
    if cookie_input:
        cred, err_msg = parse_cookie_json(cookie_input)
        if cred:
            st.success("âœ… Cookie è§£ææˆåŠŸï¼")
        else:
            st.error(f"âŒ {err_msg}")
            
    st.divider()
    
    # --- UI ä¿®æ”¹ï¼šå¢åŠ æ¨¡å¼é€‰æ‹© ---
    st.header("âš™ï¸ ä¸‹è½½è®¾ç½®")
    fetch_mode = st.radio(
        "ä¸‹è½½æ¨¡å¼",
        ("æŒ‡å®šé¡µæ•°", "å…¨éƒ¨ä¸‹è½½")
    )
    
    limit_pages = 5 # é»˜è®¤å€¼
    if fetch_mode == "æŒ‡å®šé¡µæ•°":
        limit_pages = st.slider("é€‰æ‹©æŠ“å–é¡µæ•°", 1, 100, 5)
    else:
        st.caption("âš ï¸ æ³¨æ„ï¼š'å…¨éƒ¨ä¸‹è½½'å¯èƒ½è€—æ—¶è¾ƒé•¿ï¼Œä¸”å®¹æ˜“è§¦å‘Bç«™é£æ§ï¼Œè¯·ç¡®ä¿å·²ç™»å½•Cookieã€‚")

url_input = st.text_input("ğŸ‘‡ è§†é¢‘é“¾æ¥", placeholder="https://b23.tv/...")

# === æŠ“å– ===
if st.button("å¼€å§‹æŠ“å–", type="primary"):
    if not url_input:
        st.warning("è¯·è¾“å…¥é“¾æ¥")
    else:
        bv_id, real_url = extract_bv(url_input)
        if not bv_id:
            st.error("æ— æ³•è¯†åˆ« BV å·")
        else:
            st.success(f"æ­£åœ¨æŠ“å–: {bv_id}")
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            # è°ƒç”¨ä¿®æ”¹åçš„å¹¶å‘å‡½æ•°
            title, data = loop.run_until_complete(fetch_comments_async(bv_id, fetch_mode, limit_pages, credential=cred))
            
            if isinstance(data, str):
                st.error(data)
            elif data:
                st.session_state.comments_data = data
                st.session_state.video_title = title
                st.session_state.bv_id = bv_id
                st.rerun()
            else:
                st.warning("æœªæŠ“å–åˆ°æ•°æ®ã€‚")

# === æ˜¾ç¤º ===
if st.session_state.comments_data:
    st.divider()
    
    title = st.session_state.video_title
    bv_id = st.session_state.bv_id
    data = st.session_state.comments_data
    
    st.subheader(f"ğŸ“„ {title}")
    
    df = pd.DataFrame(data)
    
    col1, col2 = st.columns([3, 1])
    
    with col2:
        st.markdown("### ğŸ› ï¸ æ•°æ®é€‰é¡¹")
        
        sort_order = st.radio(
            "æ’åºæ–¹å¼ (æŒ‰ç‚¹èµ)",
            ("é»˜è®¤ (æ—¶é—´)", "ç‚¹èµæ•° (é«˜åˆ°ä½)", "ç‚¹èµæ•° (ä½åˆ°é«˜)")
        )
        
        if sort_order == "ç‚¹èµæ•° (é«˜åˆ°ä½)":
            df = df.sort_values(by="ç‚¹èµ", ascending=False)
        elif sort_order == "ç‚¹èµæ•° (ä½åˆ°é«˜)":
            df = df.sort_values(by="ç‚¹èµ", ascending=True)
        
        st.write(f"å…±æŠ“å– {len(df)} æ¡è¯„è®º")
        
        csv = df.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½ CSV",
            data=csv,
            file_name=f"{bv_id}_comments.csv",
            mime="text/csv"
        )
        
        st.write("---")
        if st.button("ç”Ÿæˆ PDF"):
            with st.spinner("æ­£åœ¨ç”Ÿæˆ PDF (æ”¯æŒé•¿æ–‡æ¢è¡Œ)..."):
                pdf_buffer = create_pdf(df, title)
                if pdf_buffer:
                    st.success("ç”ŸæˆæˆåŠŸï¼")
                    st.download_button(
                        label="ğŸ“¥ ç‚¹å‡»ä¸‹è½½ PDF",
                        data=pdf_buffer,
                        file_name=f"{bv_id}_comments.pdf",
                        mime="application/pdf"
                    )
                else:
                    st.error("PDF ç”Ÿæˆå¤±è´¥ã€‚")

    with col1:
        st.dataframe(df, use_container_width=True, height=500)
        
    if st.button("ğŸ”„ æ¸…ç©ºç»“æœ"):
        st.session_state.comments_data = None
        st.rerun()
