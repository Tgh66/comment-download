import streamlit as st
import yt_dlp
import os
import shutil
import re
import time
import asyncio
import pandas as pd
import requests
import json
import urllib.parse
import io
import math
import zipfile
from concurrent.futures import ThreadPoolExecutor
from streamlit.runtime.scriptrunner import add_script_run_ctx, get_script_run_ctx
from bilibili_api import video, comment, Credential
from bilibili_api.exceptions import ResponseCodeException

# --- PDF ç”Ÿæˆç›¸å…³åº“ ---
from reportlab.lib.pagesizes import A4
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.cidfonts import UnicodeCIDFont 
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib import colors

# ==========================================
# 1. å…¨å±€é…ç½® (å¿…é¡»åœ¨æ‰€æœ‰ Streamlit å‘½ä»¤ä¹‹å‰)
# ==========================================
st.set_page_config(page_title="Bilibili å…¨èƒ½å·¥å…·ç®±", page_icon="ğŸ§°", layout="wide")

# ==========================================
# 2. è§†é¢‘ä¸‹è½½å™¨æ¨¡å— (Video Downloader)
# ==========================================
class VideoDownloaderApp:
    def __init__(self):
        self.DOWNLOAD_ROOT = "downloads"
        self.MAX_CONCURRENT_TASKS = 2
        self._init_environment()

    def _init_environment(self):
        if 'queue' not in st.session_state:
            st.session_state.queue = [] 
        if 'downloader_init' not in st.session_state:
            if os.path.exists(self.DOWNLOAD_ROOT):
                try:
                    shutil.rmtree(self.DOWNLOAD_ROOT)
                except:
                    pass
            os.makedirs(self.DOWNLOAD_ROOT, exist_ok=True)
            st.session_state['downloader_init'] = True

    def extract_url(self, text):
        if not text: return None
        url_pattern = r'(https?://[a-zA-Z0-9./?=&_%-]+)'
        match = re.search(url_pattern, text)
        if match: return match.group(1)
        return text

    def download_worker(self, task_info, ui_components, ctx):
        add_script_run_ctx(ctx=ctx)
        raw_url = task_info['url']
        
        # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºæ–‡ä»¶å¤¹å
        task_dir = os.path.join(self.DOWNLOAD_ROOT, f"task_{int(time.time())}_{hash(raw_url)}")
        os.makedirs(task_dir, exist_ok=True)

        def progress_hook(d):
            if d['status'] == 'downloading':
                p_str = d.get('_percent_str', '0%').replace('%', '')
                try:
                    percent = float(p_str) / 100
                    ui_components['bar'].progress(percent)
                    ui_components['status'].markdown(f"ğŸš€ ä¸‹è½½ä¸­: `{p_str}%` | âš¡ `{d.get('_speed_str')}`")
                except:
                    pass
            elif d['status'] == 'finished':
                ui_components['bar'].progress(1.0)
                ui_components['status'].markdown("âœ… ä¸‹è½½å®Œæˆï¼Œæ­£åœ¨å¤„ç†æ–‡ä»¶...")

        ydl_opts = {
            'outtmpl': f'{task_dir}/%(title)s.%(ext)s',
            'progress_hooks': [progress_hook],
            'restrictfilenames': True,
            'trim_file_name': 50,
            'format': 'bestvideo[vcodec^=avc]+bestaudio[acodec^=mp4a]/bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',
            'merge_output_format': 'mp4',
            'quiet': True,
            'no_warnings': True,
        }

        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(raw_url, download=False)
                title = info.get('title', 'Unknown')
                ui_components['title'].markdown(f"**ğŸ¬ {title}**")
                
                ydl.download([raw_url])
                time.sleep(1) 
                
                files = [f for f in os.listdir(task_dir) if f.endswith('.mp4')]
                if files:
                    file_path = os.path.join(task_dir, files[0])
                    file_size = os.path.getsize(file_path)/1024/1024
                    ui_components['status'].success(f"ğŸ‰ å®Œæˆ! ({file_size:.1f} MB)")
                    return file_path
                else:
                    ui_components['status'].error("âŒ æœªç”Ÿæˆæ–‡ä»¶")
                    return None
        except Exception as e:
            ui_components['status'].error(f"âŒ é”™è¯¯: {str(e)[:50]}...")
            return None

    def render(self):
        st.header("ğŸ›¡ï¸ Bilibili ç¨³å®šç‰ˆä¸‹è½½å™¨")
        st.caption("å·²ä¿®å¤ Windows æ’­æ”¾é—®é¢˜ & æå‡ä¸‹è½½ç¨³å®šæ€§")

        # 1. è¾“å…¥åŒº
        with st.container():
            c1, c2 = st.columns([5, 1])
            raw_input = c1.text_input("ç²˜è´´é“¾æ¥:", key="dl_url_input", placeholder="æ”¯æŒ Bilibili é“¾æ¥æˆ–åˆ†äº«å£ä»¤")
            
            def add_to_queue():
                if st.session_state.dl_url_input:
                    clean_url = self.extract_url(st.session_state.dl_url_input)
                    current_urls = [t['url'] for t in st.session_state.queue]
                    if clean_url not in current_urls:
                        st.session_state.queue.append({'url': clean_url})
                        st.toast(f"å·²æ·»åŠ ä»»åŠ¡")
                    else:
                        st.toast("ä»»åŠ¡å·²å­˜åœ¨")
            
            c2.button("â• æ·»åŠ ", on_click=add_to_queue, use_container_width=True)

        # 2. é˜Ÿåˆ—
        if st.session_state.queue:
            st.divider()
            for i, task in enumerate(st.session_state.queue):
                st.text(f"{i+1}. ğŸ”— {task['url']}")

            st.divider()

            # 3. æ‰§è¡ŒåŒº
            if st.button("ğŸš€ å¼€å§‹ä¸‹è½½", type="primary", use_container_width=True):
                st.write("---")
                
                ui_holders = []
                for i, task in enumerate(st.session_state.queue):
                    with st.container():
                        c_title = st.empty()
                        c_title.text(f"å‡†å¤‡è§£æä»»åŠ¡ {i+1}...")
                        c_bar = st.progress(0)
                        c_status = st.empty()
                        st.divider()
                        ui_holders.append({'title': c_title, 'bar': c_bar, 'status': c_status})
                
                ctx = get_script_run_ctx()
                completed_files = []
                
                with ThreadPoolExecutor(max_workers=self.MAX_CONCURRENT_TASKS) as executor:
                    futures = []
                    for i, task in enumerate(st.session_state.queue):
                        future = executor.submit(self.download_worker, task, ui_holders[i], ctx)
                        futures.append(future)
                    
                    for future in futures:
                        try:
                            res = future.result()
                            if res: completed_files.append(res)
                        except Exception:
                            pass

                if completed_files:
                    zip_name = "bilibili_videos.zip"
                    try:
                        with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zf:
                            for f in completed_files:
                                if f and os.path.exists(f):
                                    zf.write(f, os.path.basename(f))
                        
                        if os.path.exists(zip_name):
                            with open(zip_name, "rb") as f:
                                st.balloons()
                                st.download_button("ğŸ“¦ æ‰“åŒ…ä¸‹è½½æ‰€æœ‰è§†é¢‘", f, file_name=zip_name)
                    except Exception as e:
                        st.error(f"æ‰“åŒ…å¤±è´¥: {e}")

# ==========================================
# 3. è¯„è®ºæŠ“å–å™¨æ¨¡å— (Comment Scraper)
# ==========================================
class CommentScraperApp:
    def __init__(self):
        self._init_session()
        
    class VideoTypeFix:
        value = 1 

    def _init_session(self):
        if 'comments_data' not in st.session_state:
            st.session_state.comments_data = None
        if 'video_title' not in st.session_state:
            st.session_state.video_title = ""
        if 'bv_id' not in st.session_state:
            st.session_state.bv_id = ""

    def extract_bv_robust(self, text):
        if not text: return None, None
        bv_pattern = r"(BV[a-zA-Z0-9]{10})"
        match = re.search(bv_pattern, text)
        if match:
            return match.group(1), "Direct Match"
        
        short_link_pattern = r"(https?://b23\.tv/[a-zA-Z0-9]+)"
        short_match = re.search(short_link_pattern, text)
        if short_match:
            short_url = short_match.group(1)
            try:
                headers = {"User-Agent": "Mozilla/5.0"}
                resp = requests.get(short_url, headers=headers, allow_redirects=True, timeout=10)
                real_url = resp.url
                match_redirect = re.search(bv_pattern, real_url)
                if match_redirect:
                    return match_redirect.group(1), real_url
            except Exception as e:
                print(f"çŸ­é“¾æ¥è§£æå¤±è´¥: {e}")
        return None, None

    def parse_cookie_json(self, json_str):
        try:
            data = json.loads(json_str)
            cookie_list = []
            if isinstance(data, list):
                cookie_list = data
            elif isinstance(data, dict) and "cookies" in data:
                cookie_list = data["cookies"]
            else:
                return None, "JSON æ ¼å¼ä¸æ­£ç¡®"

            cookies = {c['name']: c['value'] for c in cookie_list}
            sessdata = cookies.get('SESSDATA')
            bili_jct = cookies.get('bili_jct')
            buvid3 = cookies.get('buvid3')

            if not sessdata or not bili_jct:
                return None, "ç¼ºå°‘ SESSDATA æˆ– bili_jct"

            sessdata = urllib.parse.unquote(sessdata)
            bili_jct = urllib.parse.unquote(bili_jct)
            cred = Credential(sessdata=sessdata, bili_jct=bili_jct, buvid3=buvid3)
            return cred, None
        except Exception as e:
            return None, f"è§£æé”™è¯¯: {str(e)}"

    def create_pdf(self, dataframe, title):
        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4)
        elements = []
        font_name = 'STSong-Light'
        try:
            pdfmetrics.registerFont(UnicodeCIDFont('STSong-Light'))
        except:
            font_name = "Helvetica"

        styles = getSampleStyleSheet()
        title_style = styles['Title']
        if font_name == 'STSong-Light': title_style.fontName = font_name
        
        cell_style = ParagraphStyle(name='CellStyle', fontName=font_name, fontSize=9, leading=12, wordWrap='CJK')
        safe_title = re.sub(r'[^\w\s\u4e00-\u9fa5]', '', title)
        elements.append(Paragraph(f"è§†é¢‘è¯„è®º: {safe_title}", title_style))
        elements.append(Paragraph("<br/><br/>", styles['Normal']))

        col_widths = [70, 240, 40, 80, 40] 
        headers = dataframe.columns.to_list()
        processed_data = [headers]

        for index, row in dataframe.iterrows():
            new_row = []
            uname = re.sub(r'[^\x00-\x7F\u4e00-\u9fa5]+', '', str(row['ç”¨æˆ·å']))
            content = re.sub(r'[^\x00-\x7F\u4e00-\u9fa5]+', '', str(row['å†…å®¹']))
            new_row.append(Paragraph(uname, cell_style)) 
            new_row.append(Paragraph(content, cell_style))
            new_row.append(str(row['ç‚¹èµ']))
            new_row.append(str(row['æ—¶é—´'])) 
            new_row.append(str(row['å›å¤æ•°']))
            processed_data.append(new_row)

        t = Table(processed_data, colWidths=col_widths)
        style = TableStyle([
            ('FONTNAME', (0, 0), (-1, -1), font_name), 
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, 0), 'CENTER'),
            ('VALIGN', (0, 0), (-1, -1), 'TOP'),
            ('GRID', (0, 0), (-1, -1), 0.5, colors.black),
        ])
        t.setStyle(style)
        elements.append(t)
        try:
            doc.build(elements)
            buffer.seek(0)
            return buffer
        except:
            return None

    async def fetch_one_page(self, oid, page, credential, semaphore):
        async with semaphore:
            try:
                await asyncio.sleep(0.05)
                c = await comment.get_comments(oid, self.VideoTypeFix(), page, credential=credential)
                return c
            except:
                return None

    async def fetch_comments_async(self, bv_id, fetch_mode, limit_pages, credential=None):
        v = video.Video(bvid=bv_id, credential=credential)
        try:
            info = await v.get_info()
            oid = info['aid']
            title = info['title']
        except Exception as e:
            return None, f"æ— æ³•è·å–è§†é¢‘ä¿¡æ¯: {str(e)}"

        comments_data = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        status_text.text("ğŸš€ æ­£åœ¨åˆå§‹åŒ–...")

        try:
            page_1_data = await comment.get_comments(oid, self.VideoTypeFix(), 1, credential=credential)
        except ResponseCodeException as e:
            return None, f"æŠ“å–å¤±è´¥: {e.code}"
        
        if not page_1_data: return title, []

        page_info = page_1_data.get('page', {})
        total_count = page_info.get('count', 0)
        total_pages_available = math.ceil(total_count / 20)
        
        if fetch_mode == "å…¨éƒ¨ä¸‹è½½":
            target_pages = total_pages_available
            status_text.text(f"å…± {total_count} æ¡ï¼Œçº¦ {target_pages} é¡µï¼Œå…¨é€Ÿä¸‹è½½ä¸­...")
        else:
            target_pages = min(total_pages_available, limit_pages)
            status_text.text(f"å‡†å¤‡ä¸‹è½½å‰ {target_pages} é¡µ...")

        def process_json(c_json):
            processed = []
            if 'replies' not in c_json or not c_json['replies']: return processed
            for r in c_json['replies']:
                item = {
                    'ç”¨æˆ·å': r['member']['uname'],
                    'å†…å®¹': r['content']['message'],
                    'ç‚¹èµ': int(r['like']), 
                    'æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(r['ctime'])),
                    'å›å¤æ•°': int(r['count'])
                }
                processed.append(item)
                if r.get('replies'):
                    for sub in r['replies']:
                        sub_item = {
                            'ç”¨æˆ·å': sub['member']['uname'],
                            'å†…å®¹': f"[å›å¤] {sub['content']['message']}",
                            'ç‚¹èµ': int(sub['like']),
                            'æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(sub['ctime'])),
                            'å›å¤æ•°': 0
                        }
                        processed.append(sub_item)
            return processed

        comments_data.extend(process_json(page_1_data))
        progress_bar.progress(1 / max(target_pages, 1))

        if target_pages > 1:
            sem = asyncio.Semaphore(5)
            tasks = []
            for p in range(2, target_pages + 1):
                tasks.append(self.fetch_one_page(oid, p, credential, sem))
            
            finished_count = 1
            for future in asyncio.as_completed(tasks):
                result = await future
                finished_count += 1
                if result:
                    comments_data.extend(process_json(result))
                progress_bar.progress(min(finished_count / target_pages, 1.0))
                status_text.text(f"âš¡ å¹¶å‘ä¸‹è½½ä¸­: {finished_count}/{target_pages} é¡µ")

        status_text.text("âœ… ä¸‹è½½å®Œæˆï¼")
        return title, comments_data

    def render(self):
        st.header("âš¡ Bç«™è¯„è®ºæŠ“å– (ç»ˆæç‰ˆ)")

        # ä¾§è¾¹æ é…ç½® (ä»…åœ¨è¯„è®ºæŠ“å–é¡µé¢æ˜¾ç¤º)
        with st.sidebar:
            st.divider()
            st.subheader("ğŸ“ æŠ“å–è®¾ç½®")
            cookie_input = st.text_area("Cookie JSON (å¯é€‰):", height=100, placeholder='{"cookies": [...]}')
            cred = None
            if cookie_input:
                cred, err_msg = self.parse_cookie_json(cookie_input)
                if cred: st.success("âœ… Cookie æœ‰æ•ˆ")
                else: st.error(f"âŒ {err_msg}")
            
            fetch_mode = st.radio("æ¨¡å¼", ("æŒ‡å®šé¡µæ•°", "å…¨éƒ¨ä¸‹è½½"))
            limit_pages = 5
            if fetch_mode == "æŒ‡å®šé¡µæ•°":
                limit_pages = st.slider("é¡µæ•°", 1, 100, 5)

        # ä¸»ç•Œé¢è¾“å…¥
        url_input = st.text_input("ğŸ‘‡ è§†é¢‘é“¾æ¥", placeholder="æ”¯æŒå„ç§ä¹±ç æ ¼å¼ã€çŸ­é“¾æ¥ã€ä¸­æ–‡æ ‡é¢˜æ··æ’")

        if st.button("å¼€å§‹æŠ“å–", type="primary"):
            if not url_input:
                st.warning("è¯·ç²˜è´´å†…å®¹")
            else:
                with st.spinner("æ­£åœ¨è§£æ..."):
                    bv_id, _ = self.extract_bv_robust(url_input)
                    if not bv_id:
                        st.error("æ— æ³•è¯†åˆ« BV å·")
                    else:
                        st.success(f"è¯†åˆ«æˆåŠŸ: {bv_id}")
                        try:
                            loop = asyncio.get_event_loop()
                        except RuntimeError:
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                        
                        title, data = loop.run_until_complete(self.fetch_comments_async(bv_id, fetch_mode, limit_pages, credential=cred))
                        
                        if isinstance(data, str):
                            st.error(data)
                        elif data:
                            st.session_state.comments_data = data
                            st.session_state.video_title = title
                            st.session_state.bv_id = bv_id
                            st.rerun()
                        else:
                            st.warning("æœªæŠ“å–åˆ°æ•°æ®")

        # ç»“æœæ˜¾ç¤º
        if st.session_state.comments_data:
            st.divider()
            title = st.session_state.video_title
            data = st.session_state.comments_data
            st.subheader(f"ğŸ“„ {title}")
            df = pd.DataFrame(data)
            
            col1, col2 = st.columns([3, 1])
            with col2:
                st.markdown("### ğŸ› ï¸ å¯¼å‡º")
                st.write(f"å…± {len(df)} æ¡")
                csv = df.to_csv(index=False).encode('utf-8-sig')
                st.download_button("ğŸ“¥ ä¸‹è½½ CSV", csv, f"{st.session_state.bv_id}.csv", "text/csv")
                
                if st.button("ç”Ÿæˆ PDF"):
                    with st.spinner("ç”Ÿæˆä¸­..."):
                        pdf_buffer = self.create_pdf(df, title)
                        if pdf_buffer:
                            st.download_button("ğŸ“¥ ä¸‹è½½ PDF", pdf_buffer, f"{st.session_state.bv_id}.pdf", "application/pdf")
                        else:
                            st.error("PDFç”Ÿæˆå¤±è´¥")
                            
            with col1:
                st.dataframe(df, use_container_width=True, height=500)
            
            if st.button("ğŸ”„ æ¸…ç©ºç»“æœ"):
                st.session_state.comments_data = None
                st.rerun()

# ==========================================
# 4. ä¸»ç¨‹åºå…¥å£ & å¯¼èˆª
# ==========================================

# ä¾§è¾¹æ å¯¼èˆª
st.sidebar.title("ğŸ§° Bilibili å·¥å…·ç®±")
app_mode = st.sidebar.radio(
    "é€‰æ‹©åŠŸèƒ½æ¨¡å—:",
    ["ğŸ“º è§†é¢‘ä¸‹è½½å™¨", "ğŸ“ è¯„è®ºæŠ“å–å™¨"],
    captions=["åŸºäº yt-dlp ç¨³å®šä¸‹è½½", "åŸºäº bilibili-api æŠ“å–è¯„è®º"]
)

# è·¯ç”±åˆ†å‘
if app_mode == "ğŸ“º è§†é¢‘ä¸‹è½½å™¨":
    downloader = VideoDownloaderApp()
    downloader.render()
elif app_mode == "ğŸ“ è¯„è®ºæŠ“å–å™¨":
    scraper = CommentScraperApp()
    scraper.render()

# é¡µè„š
st.sidebar.divider()
st.sidebar.caption("Provided by Gemini")
