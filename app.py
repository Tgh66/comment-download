import streamlit as st
import asyncio
import pandas as pd
import re
import time
import requests
import json
import urllib.parse
from bilibili_api import video, comment, Credential
from bilibili_api.exceptions import ResponseCodeException

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="Bç«™è¯„è®ºæŠ“å–ç¥å™¨ ", page_icon="ğŸª", layout="wide")

# --- è¾…åŠ©å‡½æ•° ---

def get_real_url(url):
    """å¤„ç† b23.tv çŸ­é“¾æ¥"""
    if "b23.tv" in url:
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            resp = requests.get(url, headers=headers, allow_redirects=True, timeout=10)
            return resp.url
        except:
            return url
    return url

def extract_bv(url):
    """æå–BVå·"""
    real_url = get_real_url(url)
    pattern = r"(BV[a-zA-Z0-9]{10})"
    match = re.search(pattern, real_url)
    if match:
        return match.group(1), real_url
    return None, real_url

def parse_cookie_json(json_str):
    """è§£æç”¨æˆ·ç²˜è´´çš„ JSON Cookie æ•°æ®"""
    try:
        data = json.loads(json_str)
        
        cookie_list = []
        if isinstance(data, list):
            cookie_list = data
        elif isinstance(data, dict) and "cookies" in data:
            cookie_list = data["cookies"]
        else:
            return None, "JSON æ ¼å¼ä¸æ­£ç¡®ï¼Œæœªæ‰¾åˆ° cookies åˆ—è¡¨"

        cookies = {c['name']: c['value'] for c in cookie_list}
        
        sessdata = cookies.get('SESSDATA')
        bili_jct = cookies.get('bili_jct')
        buvid3 = cookies.get('buvid3')

        if not sessdata or not bili_jct:
            return None, "Cookie ä¸­ç¼ºå°‘ SESSDATA æˆ– bili_jct"

        sessdata = urllib.parse.unquote(sessdata)
        bili_jct = urllib.parse.unquote(bili_jct)

        cred = Credential(sessdata=sessdata, bili_jct=bili_jct, buvid3=buvid3)
        return cred, None

    except json.JSONDecodeError:
        return None, "JSON è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥å¤åˆ¶æ˜¯å¦å®Œæ•´"
    except Exception as e:
        return None, f"Cookie è§£æé”™è¯¯: {str(e)}"

# ğŸ‘‡ ã€æ ¸å¿ƒä¿®å¤ã€‘å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰ç±»ï¼Œå®Œç¾éª—è¿‡åº“çš„æ£€æŸ¥
class VideoTypeFix:
    value = 1  # è§†é¢‘ç±»å‹ ID ä¸º 1

async def fetch_comments_async(bv_id, limit_pages, credential=None):
    """
    å¼‚æ­¥æŠ“å–è¯„è®º
    """
    v = video.Video(bvid=bv_id, credential=credential)
    
    try:
        info = await v.get_info()
        oid = info['aid']
        title = info['title']
    except Exception as e:
        return None, f"æ— æ³•è·å–è§†é¢‘ä¿¡æ¯: {str(e)}"

    comments_data = []
    page = 1
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        while page <= limit_pages:
            status_text.text(f"ğŸš€ æ­£åœ¨æŠ“å–ç¬¬ {page}/{limit_pages} é¡µ...")
            
            try:
                # ğŸ‘‡ ã€å…³é”®ä¿®æ”¹ã€‘ä½¿ç”¨è‡ªå®šä¹‰å¯¹è±¡ VideoTypeFix() ä»£æ›¿æ•°å­— 1
                c = await comment.get_comments(oid, VideoTypeFix(), page, credential=credential)
            except ResponseCodeException as e:
                if e.code == -404: break
                st.warning(f"API é”™è¯¯ä»£ç : {e.code}")
                break
            except Exception as e:
                st.warning(f"æœªçŸ¥é”™è¯¯: {e}")
                break

            if 'replies' not in c or not c['replies']:
                status_text.text("âœ… å·²åˆ°è¾¾åº•éƒ¨")
                break
            
            for r in c['replies']:
                item = {
                    'ç”¨æˆ·å': r['member']['uname'],
                    'å†…å®¹': r['content']['message'],
                    'ç‚¹èµ': r['like'],
                    'æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(r['ctime'])),
                    'å›å¤æ•°': r['count']
                }
                comments_data.append(item)
                
                if r.get('replies'):
                    for sub in r['replies']:
                        sub_item = {
                            'ç”¨æˆ·å': sub['member']['uname'],
                            'å†…å®¹': f"[å›å¤] {sub['content']['message']}",
                            'ç‚¹èµ': sub['like'],
                            'æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(sub['ctime'])),
                            'å›å¤æ•°': 0
                        }
                        comments_data.append(sub_item)

            progress_bar.progress(min(page / limit_pages, 1.0))
            page += 1
            await asyncio.sleep(0.5)
            
    except Exception as e:
        st.error(f"ä¸­æ–­: {e}")
    
    return title, comments_data

# --- UI å¸ƒå±€ ---

st.title("ğŸª Bç«™è¯„è®ºæŠ“å– (å¼ºåŠ›ä¿®å¤ç‰ˆ)")

with st.sidebar:
    st.header("ğŸ” èº«ä»½éªŒè¯ (æ¨è)")
    st.info("ç²˜è´´ Cookie JSON")
    
    cookie_input = st.text_area(
        "Cookie æ•°æ®:", 
        height=150,
        placeholder='{"url": "...", "cookies": [...]}'
    )
    
    cred = None
    if cookie_input:
        cred, err_msg = parse_cookie_json(cookie_input)
        if cred:
            st.success("âœ… Cookie è§£ææˆåŠŸï¼")
        else:
            st.error(f"âŒ {err_msg}")
            
    st.divider()
    max_pages = st.slider("æŠ“å–é¡µæ•°", 1, 100, 5)

url_input = st.text_input("ğŸ‘‡ è§†é¢‘é“¾æ¥", placeholder="https://b23.tv/...")

if st.button("å¼€å§‹æŠ“å–", type="primary"):
    if not url_input:
        st.warning("è¯·è¾“å…¥é“¾æ¥")
    else:
        bv_id, real_url = extract_bv(url_input)
        if not bv_id:
            st.error("æ— æ³•è¯†åˆ« BV å·")
        else:
            st.success(f"æ­£åœ¨æŠ“å–: {bv_id}")
            
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            title, data = loop.run_until_complete(fetch_comments_async(bv_id, max_pages, credential=cred))
            
            if isinstance(data, str):
                st.error(data)
            elif data:
                st.subheader(f"ğŸ“„ {title}")
                df = pd.DataFrame(data)
                st.dataframe(df, use_container_width=True)
                
                csv = df.to_csv(index=False).encode('utf-8-sig')
                st.download_button("ğŸ“¥ ä¸‹è½½æ•°æ® (CSV)", csv, f"{bv_id}.csv", "text/csv")
            else:
                st.warning("æœªæŠ“å–åˆ°æ•°æ®ã€‚")
