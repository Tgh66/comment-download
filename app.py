import streamlit as st
import asyncio
import pandas as pd
import re
import time
import requests
import json
import urllib.parse
import io 
import os
import math
from bilibili_api import video, comment, Credential
from bilibili_api.exceptions import ResponseCodeException

# --- PDF ç”Ÿæˆç›¸å…³åº“ ---
from reportlab.lib.pagesizes import A4
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.cidfonts import UnicodeCIDFont 
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib import colors

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="Bç«™è¯„è®ºæŠ“å–ç¥å™¨ (ç»ˆæç‰ˆ)", page_icon="ğŸ”¥", layout="wide")

# --- åˆå§‹åŒ– Session State ---
if 'comments_data' not in st.session_state:
    st.session_state.comments_data = None
if 'video_title' not in st.session_state:
    st.session_state.video_title = ""
if 'bv_id' not in st.session_state:
    st.session_state.bv_id = ""

# --- æ ¸å¿ƒè¾…åŠ©å‡½æ•° (URLè§£æå¢å¼ºç‰ˆ) ---

def extract_bv_robust(text):
    """
    ç»ˆæ BV å·æå–å‡½æ•°ï¼Œæ”¯æŒï¼š
    1. æ ‡å‡†é“¾æ¥
    2. å¸¦æœ‰ä¸­æ–‡æ ‡é¢˜çš„æ··åˆæ–‡æœ¬
    3. b23.tv çŸ­é“¾æ¥ (è‡ªåŠ¨è§£æè·³è½¬)
    4. æ ¼å¼é”™è¯¯çš„é“¾æ¥ (å¦‚ http://1https//...)
    """
    if not text:
        return None, None

    # 1. å°è¯•ç›´æ¥æ­£åˆ™åŒ¹é… BV å· (æœ€å¿«ï¼Œæœ€å‡†)
    # åªè¦å­—ç¬¦ä¸²é‡ŒåŒ…å« BV.......... å°±èƒ½åŒ¹é…åˆ°ï¼Œå¿½ç•¥å‘¨å›´çš„ä¹±ç 
    bv_pattern = r"(BV[a-zA-Z0-9]{10})"
    match = re.search(bv_pattern, text)
    
    if match:
        return match.group(1), "Direct Match"

    # 2. å¦‚æœæ²¡æ‰¾åˆ° BV å·ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å« b23.tv çŸ­é“¾æ¥
    # æå– text ä¸­çš„ http...b23.tv... éƒ¨åˆ†
    short_link_pattern = r"(https?://b23\.tv/[a-zA-Z0-9]+)"
    short_match = re.search(short_link_pattern, text)
    
    if short_match:
        short_url = short_match.group(1)
        try:
            # æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®çŸ­é“¾æ¥ï¼Œè·å–é‡å®šå‘åçš„çœŸå® URL
            headers = {"User-Agent": "Mozilla/5.0"}
            resp = requests.get(short_url, headers=headers, allow_redirects=True, timeout=10)
            real_url = resp.url
            
            # ä»è·³è½¬åçš„ URL ä¸­å†æ¬¡æŸ¥æ‰¾ BV å·
            match_redirect = re.search(bv_pattern, real_url)
            if match_redirect:
                return match_redirect.group(1), real_url
        except Exception as e:
            print(f"çŸ­é“¾æ¥è§£æå¤±è´¥: {e}")
            return None, None

    return None, None

def parse_cookie_json(json_str):
    """è§£æç”¨æˆ·ç²˜è´´çš„ JSON Cookie æ•°æ®"""
    try:
        data = json.loads(json_str)
        
        cookie_list = []
        if isinstance(data, list):
            cookie_list = data
        elif isinstance(data, dict) and "cookies" in data:
            cookie_list = data["cookies"]
        else:
            return None, "JSON æ ¼å¼ä¸æ­£ç¡®ï¼Œæœªæ‰¾åˆ° cookies åˆ—è¡¨"

        cookies = {c['name']: c['value'] for c in cookie_list}
        
        sessdata = cookies.get('SESSDATA')
        bili_jct = cookies.get('bili_jct')
        buvid3 = cookies.get('buvid3')

        if not sessdata or not bili_jct:
            return None, "Cookie ä¸­ç¼ºå°‘ SESSDATA æˆ– bili_jct"

        sessdata = urllib.parse.unquote(sessdata)
        bili_jct = urllib.parse.unquote(bili_jct)

        cred = Credential(sessdata=sessdata, bili_jct=bili_jct, buvid3=buvid3)
        return cred, None

    except json.JSONDecodeError:
        return None, "JSON è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥å¤åˆ¶æ˜¯å¦å®Œæ•´"
    except Exception as e:
        return None, f"Cookie è§£æé”™è¯¯: {str(e)}"

# --- PDF ç”Ÿæˆå‡½æ•° ---
def create_pdf(dataframe, title):
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4)
    elements = []

    font_name = 'STSong-Light'
    try:
        pdfmetrics.registerFont(UnicodeCIDFont('STSong-Light'))
    except Exception as e:
        font_name = "Helvetica"

    styles = getSampleStyleSheet()
    title_style = styles['Title']
    if font_name == 'STSong-Light':
        title_style.fontName = font_name
    
    cell_style = ParagraphStyle(
        name='CellStyle',
        fontName=font_name,
        fontSize=9,
        leading=12,
        wordWrap='CJK'
    )

    safe_title = re.sub(r'[^\w\s\u4e00-\u9fa5]', '', title)
    elements.append(Paragraph(f"è§†é¢‘è¯„è®º: {safe_title}", title_style))
    elements.append(Paragraph("<br/><br/>", styles['Normal']))

    col_widths = [70, 240, 40, 80, 40] 
    headers = dataframe.columns.to_list()
    processed_data = [headers]

    for index, row in dataframe.iterrows():
        new_row = []
        uname = str(row['ç”¨æˆ·å'])
        content = str(row['å†…å®¹'])
        like = str(row['ç‚¹èµ'])
        time_str = str(row['æ—¶é—´'])
        reply_count = str(row['å›å¤æ•°'])

        content = re.sub(r'[^\x00-\x7F\u4e00-\u9fa5]+', '', content)
        uname = re.sub(r'[^\x00-\x7F\u4e00-\u9fa5]+', '', uname)

        new_row.append(Paragraph(uname, cell_style)) 
        new_row.append(Paragraph(content, cell_style))
        new_row.append(like)
        new_row.append(time_str) 
        new_row.append(reply_count)
        processed_data.append(new_row)

    t = Table(processed_data, colWidths=col_widths)
    style = TableStyle([
        ('FONTNAME', (0, 0), (-1, -1), font_name), 
        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
        ('ALIGN', (0, 0), (-1, 0), 'CENTER'),
        ('VALIGN', (0, 0), (-1, -1), 'TOP'),
        ('FONTSIZE', (0, 0), (-1, 0), 10),
        ('FONTSIZE', (0, 0), (-1, -1), 9),
        ('GRID', (0, 0), (-1, -1), 0.5, colors.black),
        ('LEFTPADDING', (0, 0), (-1, -1), 4),
        ('RIGHTPADDING', (0, 0), (-1, -1), 4),
        ('TOPPADDING', (0, 0), (-1, -1), 4),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 4),
    ])
    t.setStyle(style)
    elements.append(t)

    try:
        doc.build(elements)
        buffer.seek(0)
        return buffer
    except Exception as e:
        print(f"PDFç”Ÿæˆé”™è¯¯: {e}")
        return None

# ğŸ‘‡ è‡ªå®šä¹‰ç±»
class VideoTypeFix:
    value = 1 

# --- å¼‚æ­¥å¹¶å‘æŠ“å–é€»è¾‘ ---
async def fetch_one_page(oid, page, credential, semaphore):
    async with semaphore:
        try:
            await asyncio.sleep(0.05)
            c = await comment.get_comments(oid, VideoTypeFix(), page, credential=credential)
            return c
        except Exception as e:
            return None

async def fetch_comments_async(bv_id, fetch_mode, limit_pages, credential=None):
    v = video.Video(bvid=bv_id, credential=credential)
    
    try:
        info = await v.get_info()
        oid = info['aid']
        title = info['title']
    except Exception as e:
        return None, f"æ— æ³•è·å–è§†é¢‘ä¿¡æ¯: {str(e)}"

    comments_data = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    status_text.text("ğŸš€ æ­£åœ¨åˆå§‹åŒ–...")

    # 1. æŠ“å–ç¬¬1é¡µ
    try:
        page_1_data = await comment.get_comments(oid, VideoTypeFix(), 1, credential=credential)
    except ResponseCodeException as e:
        return None, f"æŠ“å–ç¬¬1é¡µå¤±è´¥ï¼Œé”™è¯¯ç : {e.code}"
    
    if not page_1_data:
        return title, []

    page_info = page_1_data.get('page', {})
    total_count = page_info.get('count', 0)
    total_pages_available = math.ceil(total_count / 20)
    
    if fetch_mode == "å…¨éƒ¨ä¸‹è½½":
        target_pages = total_pages_available
        status_text.text(f"å…± {total_count} æ¡è¯„è®ºï¼Œçº¦ {target_pages} é¡µï¼Œå…¨é€Ÿä¸‹è½½ä¸­...")
    else:
        target_pages = min(total_pages_available, limit_pages)
        status_text.text(f"å‡†å¤‡ä¸‹è½½å‰ {target_pages} é¡µ...")

    def process_comments_json(c_json):
        processed = []
        if 'replies' not in c_json or not c_json['replies']:
            return processed
        for r in c_json['replies']:
            item = {
                'ç”¨æˆ·å': r['member']['uname'],
                'å†…å®¹': r['content']['message'],
                'ç‚¹èµ': int(r['like']), 
                'æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(r['ctime'])),
                'å›å¤æ•°': int(r['count'])
            }
            processed.append(item)
            if r.get('replies'):
                for sub in r['replies']:
                    sub_item = {
                        'ç”¨æˆ·å': sub['member']['uname'],
                        'å†…å®¹': f"[å›å¤] {sub['content']['message']}",
                        'ç‚¹èµ': int(sub['like']),
                        'æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(sub['ctime'])),
                        'å›å¤æ•°': 0
                    }
                    processed.append(sub_item)
        return processed

    comments_data.extend(process_comments_json(page_1_data))
    progress_bar.progress(1 / max(target_pages, 1))

    # 2. å¹¶å‘åç»­é¡µé¢
    if target_pages > 1:
        sem = asyncio.Semaphore(5)
        tasks = []
        for p in range(2, target_pages + 1):
            task = fetch_one_page(oid, p, credential, sem)
            tasks.append(task)
        
        finished_count = 1
        for future in asyncio.as_completed(tasks):
            result = await future
            finished_count += 1
            if result:
                new_items = process_comments_json(result)
                comments_data.extend(new_items)
            progress = min(finished_count / target_pages, 1.0)
            progress_bar.progress(progress)
            status_text.text(f"âš¡ å¹¶å‘ä¸‹è½½ä¸­: {finished_count}/{target_pages} é¡µ")

    status_text.text("âœ… ä¸‹è½½å®Œæˆï¼")
    await asyncio.sleep(0.5)
    return title, comments_data

# --- UI å¸ƒå±€ ---

st.title("âš¡ Bç«™è¯„è®ºæŠ“å– (ç»ˆæç‰ˆ)")

with st.sidebar:
    st.header("ğŸ” èº«ä»½éªŒè¯ (æ¨è)")
    st.info("ç²˜è´´ Cookie JSON")
    cookie_input = st.text_area("Cookie æ•°æ®:", height=150, placeholder='{"url": "...", "cookies": [...]}')
    
    cred = None
    if cookie_input:
        cred, err_msg = parse_cookie_json(cookie_input)
        if cred:
            st.success("âœ… Cookie è§£ææˆåŠŸï¼")
        else:
            st.error(f"âŒ {err_msg}")
            
    st.divider()
    st.header("âš™ï¸ ä¸‹è½½è®¾ç½®")
    fetch_mode = st.radio("ä¸‹è½½æ¨¡å¼", ("æŒ‡å®šé¡µæ•°", "å…¨éƒ¨ä¸‹è½½"))
    
    limit_pages = 5
    if fetch_mode == "æŒ‡å®šé¡µæ•°":
        limit_pages = st.slider("é€‰æ‹©æŠ“å–é¡µæ•°", 1, 100, 5)

# ä¼˜åŒ–çš„è¾“å…¥æç¤º
url_input = st.text_input(
    "ğŸ‘‡ è§†é¢‘é“¾æ¥ (æ”¯æŒå„ç§ä¹±ç æ ¼å¼ã€çŸ­é“¾æ¥ã€ä¸­æ–‡æ ‡é¢˜æ··æ’)", 
    placeholder="ç›´æ¥ç²˜è´´å¤åˆ¶çš„å†…å®¹ï¼Œä¾‹å¦‚ï¼šã€è§†é¢‘æ ‡é¢˜ã€‘ https://b23.tv/..."
)

# === æŠ“å– ===
if st.button("å¼€å§‹æŠ“å–", type="primary"):
    if not url_input:
        st.warning("è¯·ç²˜è´´å†…å®¹")
    else:
        # ä½¿ç”¨æ–°çš„é²æ£’æå–å‡½æ•°
        with st.spinner("æ­£åœ¨è§£æé“¾æ¥..."):
            bv_id, _ = extract_bv_robust(url_input)
            
        if not bv_id:
            st.error("æ— æ³•è¯†åˆ« BV å·ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ˜¯å¦æœ‰æ•ˆ")
        else:
            st.success(f"è¯†åˆ«æˆåŠŸ: {bv_id}")
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            title, data = loop.run_until_complete(fetch_comments_async(bv_id, fetch_mode, limit_pages, credential=cred))
            
            if isinstance(data, str):
                st.error(data)
            elif data:
                st.session_state.comments_data = data
                st.session_state.video_title = title
                st.session_state.bv_id = bv_id
                st.rerun()
            else:
                st.warning("æœªæŠ“å–åˆ°æ•°æ®ã€‚")

# === æ˜¾ç¤º ===
if st.session_state.comments_data:
    st.divider()
    
    title = st.session_state.video_title
    bv_id = st.session_state.bv_id
    data = st.session_state.comments_data
    
    st.subheader(f"ğŸ“„ {title}")
    
    df = pd.DataFrame(data)
    
    col1, col2 = st.columns([3, 1])
    
    with col2:
        st.markdown("### ğŸ› ï¸ æ•°æ®é€‰é¡¹")
        sort_order = st.radio("æ’åºæ–¹å¼ (æŒ‰ç‚¹èµ)", ("é»˜è®¤ (æ—¶é—´)", "ç‚¹èµæ•° (é«˜åˆ°ä½)", "ç‚¹èµæ•° (ä½åˆ°é«˜)"))
        
        if sort_order == "ç‚¹èµæ•° (é«˜åˆ°ä½)":
            df = df.sort_values(by="ç‚¹èµ", ascending=False)
        elif sort_order == "ç‚¹èµæ•° (ä½åˆ°é«˜)":
            df = df.sort_values(by="ç‚¹èµ", ascending=True)
        
        st.write(f"å…±æŠ“å– {len(df)} æ¡è¯„è®º")
        
        csv = df.to_csv(index=False).encode('utf-8-sig')
        st.download_button(label="ğŸ“¥ ä¸‹è½½ CSV", data=csv, file_name=f"{bv_id}_comments.csv", mime="text/csv")
        
        st.write("---")
        if st.button("ç”Ÿæˆ PDF"):
            with st.spinner("æ­£åœ¨ç”Ÿæˆ PDF..."):
                pdf_buffer = create_pdf(df, title)
                if pdf_buffer:
                    st.success("ç”ŸæˆæˆåŠŸï¼")
                    st.download_button(label="ğŸ“¥ ç‚¹å‡»ä¸‹è½½ PDF", data=pdf_buffer, file_name=f"{bv_id}_comments.pdf", mime="application/pdf")
                else:
                    st.error("PDF ç”Ÿæˆå¤±è´¥ã€‚")

    with col1:
        st.dataframe(df, use_container_width=True, height=500)
        
    if st.button("ğŸ”„ æ¸…ç©ºç»“æœ"):
        st.session_state.comments_data = None
        st.rerun()
